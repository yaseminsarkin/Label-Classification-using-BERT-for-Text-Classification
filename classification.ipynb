{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11278270,"sourceType":"datasetVersion","datasetId":7051021},{"sourceId":11278276,"sourceType":"datasetVersion","datasetId":7051027},{"sourceId":11278284,"sourceType":"datasetVersion","datasetId":7051035}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os \nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup,  DistilBertForSequenceClassification,AutoModelForMaskedLM\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, random_split \nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:44:49.343888Z","iopub.execute_input":"2025-04-04T18:44:49.344186Z","iopub.status.idle":"2025-04-04T18:45:09.647966Z","shell.execute_reply.started":"2025-04-04T18:44:49.344163Z","shell.execute_reply":"2025-04-04T18:45:09.646925Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:45:21.839010Z","iopub.execute_input":"2025-04-04T18:45:21.839630Z","iopub.status.idle":"2025-04-04T18:45:21.843358Z","shell.execute_reply.started":"2025-04-04T18:45:21.839596Z","shell.execute_reply":"2025-04-04T18:45:21.842467Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"traindata = pd.read_csv('')\ntestdata = pd.read_csv('')\nvaldata = pd.read_csv('')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:47:02.328700Z","iopub.execute_input":"2025-04-04T18:47:02.329096Z","iopub.status.idle":"2025-04-04T18:47:02.377905Z","shell.execute_reply.started":"2025-04-04T18:47:02.329059Z","shell.execute_reply":"2025-04-04T18:47:02.377210Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"label2ids = {'Positive': 1, 'Negative': 0, 'Neutral': 2}\ntraindata.labels = traindata.labels.apply(lambda x: label2ids[x])\ntestdata.labels = testdata.labels.apply(lambda x: label2ids[x])\nvaldata.labels = valdata.labels.apply(lambda x: label2ids[x])\n \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMODEL_NAME = 'dbmdz/bert-base-turkish-cased'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label2ids))\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:48:48.301526Z","iopub.execute_input":"2025-04-04T18:48:48.301938Z","iopub.status.idle":"2025-04-04T18:48:52.640388Z","shell.execute_reply.started":"2025-04-04T18:48:48.301907Z","shell.execute_reply":"2025-04-04T18:48:52.639580Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5c2c7f0cb024d83ba3046e8d7a84e83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea9d37db31aa4d339f77eb80487ea789"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d66941fbdd4819b5829567c5053b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9073924cf2594aeca4ca402a45c0d53f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def encode(dataset):\n    input_ids = []\n    attention_masks = []\n\n    for text in tqdm(dataset.text.values, total=dataset.shape[0]): \n        encoded_text = tokenizer(text, add_special_tokens=True, return_tensors='pt', truncation=True, padding='max_length')\n        input_ids.append(encoded_text['input_ids'])\n        attention_masks.append(encoded_text['attention_mask'])\n\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(dataset.labels.values)\n\n    return TensorDataset(input_ids, attention_masks, labels)\n\nBATCH_SIZE = 16\ntrain_loader = DataLoader(encode(traindata), batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(encode(testdata), batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(encode(valdata),  batch_size=BATCH_SIZE, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:49:18.565357Z","iopub.execute_input":"2025-04-04T18:49:18.565667Z","iopub.status.idle":"2025-04-04T18:49:22.871439Z","shell.execute_reply.started":"2025-04-04T18:49:18.565644Z","shell.execute_reply":"2025-04-04T18:49:22.870696Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 6059/6059 [00:03<00:00, 1835.64it/s]\n100%|██████████| 1189/1189 [00:00<00:00, 2035.91it/s]\n100%|██████████| 674/674 [00:00<00:00, 2068.94it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\n\ntraining_stats = []\nNUM_EPOCHS = 1\n\nfor epoch in tqdm(range(NUM_EPOCHS)):\n    model.train()\n    total_correct_predictions_train = 0\n    total_epoch_loss_train = 0\n    steps = 0\n\n    for batch in train_loader:\n        input_ids, attention_masks, labels = [r.to(\"cuda\") for r in batch]\n\n        outputs = model(input_ids, attention_mask=attention_masks)\n        logits = outputs.logits\n        loss = criterion(logits, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        steps += 1\n        predictions = torch.argmax(logits, dim=1)\n        total_correct_predictions_train += (predictions == labels).sum().item()\n        total_epoch_loss_train += loss.item()\n\n    avg_train_loss = total_epoch_loss_train / steps\n    train_accuracy = total_correct_predictions_train / len(train_loader.dataset)\n    print(f'Epoch {epoch + 1} Training Accuracy: {train_accuracy:.4f}, Average Loss: {avg_train_loss:.4f}')\n\n    # Validation\n    model.eval()\n    total_correct_predictions_val = 0\n    total_eval_loss_val = 0\n    eval_steps = 0\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids, attention_masks, labels = [r.to(\"cuda\") for r in batch]\n\n            outputs = model(input_ids, attention_mask=attention_masks)\n            logits = outputs.logits\n            loss = criterion(logits, labels)\n            predictions = torch.argmax(logits, dim=1)\n\n            total_correct_predictions_val += (predictions == labels).sum().item()\n            total_eval_loss_val += loss.item()\n            eval_steps += 1\n\n    avg_val_loss = total_eval_loss_val / eval_steps\n    val_accuracy = total_correct_predictions_val / len(val_loader.dataset)\n    print(f'Epoch {epoch + 1} Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}')\n\n    training_stats.append({\n        'epoch': epoch + 1,\n        'Training Loss': avg_train_loss,\n        'Valid. Loss': avg_val_loss,\n        'Valid. Accur.': val_accuracy\n    })\n\n    print(f'Epoch: {epoch + 1} / {NUM_EPOCHS} - Average Training Loss: {avg_train_loss:.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T18:51:35.036152Z","iopub.execute_input":"2025-04-04T18:51:35.036449Z","iopub.status.idle":"2025-04-04T19:01:33.052225Z","shell.execute_reply.started":"2025-04-04T18:51:35.036428Z","shell.execute_reply":"2025-04-04T19:01:33.051415Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Training Accuracy: 0.7924, Average Loss: 0.5239\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [09:58<00:00, 598.00s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Validation Accuracy: 0.8175, Validation Loss: 0.5124\nEpoch: 1 / 1 - Average Training Loss: 0.5239\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model.eval()\ncorrect_predictions = 0\ntotal_predictions = 0\n\nall_predictions = []\nall_labels = []\n\nfor batch in test_loader:\n    batch = [r.to(\"cuda\") for r in batch]\n    input_ids, attention_masks, labels = batch\n    \n    all_labels.extend(labels.cpu().numpy())\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_masks)\n        \n    predictions = torch.argmax(outputs.logits, dim=1) \n    all_predictions.extend(predictions.cpu().numpy())  \n\n    correct_predictions += (predictions == labels).sum().item()\n    total_predictions += labels.size(0)\n\naccuracy = (correct_predictions / total_predictions) * 100\nprint(f\"\\nTEST ACCURACY: {accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:06:23.671112Z","iopub.execute_input":"2025-04-04T19:06:23.671425Z","iopub.status.idle":"2025-04-04T19:06:57.273946Z","shell.execute_reply.started":"2025-04-04T19:06:23.671404Z","shell.execute_reply":"2025-04-04T19:06:57.273010Z"}},"outputs":[{"name":"stdout","text":"\nTEST ACCURACY: 82.25%\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nimport numpy as np\n\nflat_true_labels = np.array(all_labels)\nflat_pred_labels = np.array(all_predictions)\nf1 = f1_score(flat_true_labels, flat_pred_labels, average='weighted')\nprint(f\"F1 Score: {f1:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:11:01.695579Z","iopub.execute_input":"2025-04-04T19:11:01.696016Z","iopub.status.idle":"2025-04-04T19:11:01.711798Z","shell.execute_reply.started":"2025-04-04T19:11:01.695983Z","shell.execute_reply":"2025-04-04T19:11:01.711002Z"}},"outputs":[{"name":"stdout","text":"F1 Score: 0.82\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"output_directory = \"\"\nos.makedirs(output_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:12:43.274712Z","iopub.execute_input":"2025-04-04T19:12:43.275068Z","iopub.status.idle":"2025-04-04T19:12:43.279240Z","shell.execute_reply.started":"2025-04-04T19:12:43.275044Z","shell.execute_reply":"2025-04-04T19:12:43.278299Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import GenerationConfig\n\n# Creating the GenerationConfig with a max length of 128\ngeneration_config = GenerationConfig(max_length=128)\n\n# Save the GenerationConfig to the specified directory\ngeneration_config.save_pretrained(output_directory)\n\n# Ensure that all model parameters are contiguous (efficient memory layout)\nfor name, param in model.named_parameters():\n    if not param.is_contiguous():\n        param.data = param.data.contiguous()\n\n# save the actual model \nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained(output_directory)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T19:21:51.604032Z","iopub.execute_input":"2025-04-04T19:21:51.604385Z","iopub.status.idle":"2025-04-04T19:21:53.092528Z","shell.execute_reply.started":"2025-04-04T19:21:51.604362Z","shell.execute_reply":"2025-04-04T19:21:53.091542Z"}},"outputs":[],"execution_count":17}]}